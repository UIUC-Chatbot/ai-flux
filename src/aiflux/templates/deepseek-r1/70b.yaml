# DeepSeek R1 70B Configuration
name: "deepseek-r1:70b"

resources:
  gpu_layers: 64
  gpu_memory: "48GB"
  batch_size: 4
  max_concurrent: 1

parameters:
  temperature: 0.7
  top_p: 0.9
  max_tokens: 2048
  stop_sequences: ["<|begin▁of▁sentence|>", "<|end▁of▁sentence|>"]  # Default stop sequences from Ollama

system:
  prompt: "You are a helpful AI assistant. You are direct, accurate, and helpful in your responses."

validation:
  temperature_range: [0.0, 1.0]
  max_tokens_limit: 4096
  batch_size_range: [1, 8]
  concurrent_range: [1, 2]

# Resource Requirements
requirements:
  min_gpu_memory: "48GB"  # Minimum required for safe operation
  recommended_gpu: "A100-80GB"
  cuda_version: ">=12.0"
  cpu_threads: 8
  gpu_memory_utilization: 0.9 